{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vaibhavs10/notebooks/blob/main/orpheus-pretrained-inference-demo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"canopylabs/orpheus-3b-0.1-pretrained\"\n",
        "\n",
        "print(\"*** Change the model you use here\")"
      ],
      "metadata": {
        "id": "my_UA_HRu2tK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae9d7e55-dcad-481a-8b8c-b1d71ad5565c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Change the model you use here\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lHWzRUCDcyMx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title Installation & Setup\n",
        "%%capture\n",
        "!pip install snac ipywebrtc\n",
        "!pip install datasets\n",
        "from snac import SNAC\n",
        "import torch\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, Trainer, TrainingArguments, AutoTokenizer\n",
        "import numpy as np\n",
        "import soundfile as sf\n",
        "import IPython.display as ipd\n",
        "import librosa\n",
        "from ipywebrtc import AudioRecorder, Audio\n",
        "from IPython.display import display\n",
        "import ipywidgets as widgets\n",
        "from huggingface_hub import snapshot_download\n",
        "import torchaudio.transforms as T\n",
        "import librosa\n",
        "import torch\n",
        "from IPython.display import Audio, display\n",
        "\n",
        "model_name = \"canopylabs/orpheus-tts-0.1-pretrained\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "snac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\n",
        "\n",
        "\n",
        "# Download only model config and safetensors\n",
        "model_path = snapshot_download(\n",
        "    repo_id=model_name,\n",
        "    allow_patterns=[\n",
        "        \"config.json\",\n",
        "        \"*.safetensors\",\n",
        "        \"model.safetensors.index.json\",\n",
        "    ],\n",
        "    ignore_patterns=[\n",
        "        \"optimizer.pt\",\n",
        "        \"pytorch_model.bin\",\n",
        "        \"training_args.bin\",\n",
        "        \"scheduler.pt\",\n",
        "        \"tokenizer.json\",\n",
        "        \"tokenizer_config.json\",\n",
        "        \"special_tokens_map.json\",\n",
        "        \"vocab.json\",\n",
        "        \"merges.txt\",\n",
        "        \"tokenizer.*\"\n",
        "    ]\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.bfloat16)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### CHANGE THIS TO YOUR OWN FILE AND TEXT\n",
        "\n",
        "my_wav_file_is = \"X.wav\"\n",
        "and_the_transcript_is = \"Something or the other\"\n",
        "\n",
        "the_model_should_say = [\n",
        "  \"I finally got into the university of my dreams! I can't believe all this hard work actually  paid off!\",\n",
        "  \"Why is your frickin' Waymo blocking the frickin' road? GET OUT OF THE WAY!\",\n",
        "  \"I'm so sorry to hear about your pet, but you know, he'll pull through.\",\n",
        "  \"Conversational, uhm, systems, tend to speak pretty robotically, because- because they don't, really understand how, uhm, humans talk.\"\n",
        "\n",
        "]"
      ],
      "metadata": {
        "id": "P81EElEWvg2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Tokenising your stuff for the prompt\n",
        "%%capture\n",
        "\n",
        "''' Here we tokenise the prompt you gave us, we also tokenise the prompts you want the model to say\n",
        "\n",
        "The template is:\n",
        "\n",
        "start_of_human, start_of_text, text, end_of_text, start_of_ai, start_of_speech, speech, end_of_speech, end_of_ai, start_of_human, text, end_of_human and then generate from here\n",
        "\n",
        "'''\n",
        "\n",
        "\n",
        "filename = my_wav_file_is\n",
        "\n",
        "audio_array, sample_rate = librosa.load(filename, sr=24000)\n",
        "\n",
        "def tokenise_audio(waveform):\n",
        "  waveform = torch.from_numpy(waveform).unsqueeze(0)\n",
        "  waveform = waveform.to(dtype=torch.float32)\n",
        "\n",
        "\n",
        "  waveform = waveform.unsqueeze(0)\n",
        "\n",
        "  with torch.inference_mode():\n",
        "    codes = snac_model.encode(waveform)\n",
        "\n",
        "  all_codes = []\n",
        "  for i in range(codes[0].shape[1]):\n",
        "    all_codes.append(codes[0][0][i].item()+128266)\n",
        "    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n",
        "    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n",
        "    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n",
        "    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n",
        "\n",
        "\n",
        "  return all_codes\n",
        "\n",
        "myts = tokenise_audio(audio_array)\n",
        "start_tokens = torch.tensor([[ 128259]], dtype=torch.int64)\n",
        "end_tokens = torch.tensor([[128009, 128260, 128261, 128257]], dtype=torch.int64)\n",
        "final_tokens = torch.tensor([[128258, 128262]], dtype=torch.int64)\n",
        "voice_prompt = and_the_transcript_is\n",
        "prompt_tokked = tokenizer(voice_prompt, return_tensors=\"pt\")\n",
        "\n",
        "input_ids = prompt_tokked[\"input_ids\"]\n",
        "\n",
        "zeroprompt_input_ids = torch.cat([start_tokens, input_ids, end_tokens, torch.tensor([myts]), final_tokens], dim=1) # SOH SOT Text EOT EOH\n",
        "\n",
        "prompts = the_model_should_say\n",
        "\n",
        "all_modified_input_ids = []\n",
        "for prompt in prompts:\n",
        "  input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
        "  second_input_ids = torch.cat([zeroprompt_input_ids, start_tokens, input_ids, end_tokens], dim=1)\n",
        "  all_modified_input_ids.append(second_input_ids)\n",
        "\n",
        "\n",
        "all_padded_tensors = []\n",
        "all_attention_masks = []\n",
        "\n",
        "max_length = max([modified_input_ids.shape[1] for modified_input_ids in all_modified_input_ids])\n",
        "\n",
        "for modified_input_ids in all_modified_input_ids:\n",
        "  padding = max_length - modified_input_ids.shape[1]\n",
        "  padded_tensor = torch.cat([torch.full((1, padding), 128263, dtype=torch.int64), modified_input_ids], dim=1)\n",
        "  attention_mask = torch.cat([torch.zeros((1, padding), dtype=torch.int64), torch.ones((1, modified_input_ids.shape[1]), dtype=torch.int64)], dim=1)\n",
        "  all_padded_tensors.append(padded_tensor)\n",
        "  all_attention_masks.append(attention_mask)\n",
        "\n",
        "all_padded_tensors = torch.cat(all_padded_tensors, dim=0)\n",
        "all_attention_masks = torch.cat(all_attention_masks, dim=0)\n",
        "\n",
        "input_ids = all_padded_tensors.to(\"cuda\")\n",
        "attention_mask = all_attention_masks.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "cXgZmdclbfk_",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J_D2LtYw9gkl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b937d8-720f-4445-ebdd-70063a065e6b",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:128258 for open-end generation.\n"
          ]
        }
      ],
      "source": [
        "#@title Run Inference\n",
        "\n",
        "with torch.no_grad():\n",
        "  generated_ids = model.generate(\n",
        "      input_ids=input_ids,\n",
        "      # attention_mask=attention_mask,\n",
        "      max_new_tokens=990,\n",
        "      do_sample=True,\n",
        "      temperature=0.5,\n",
        "      # top_k=40,\n",
        "      top_p=0.9,\n",
        "      repetition_penalty=1.1,\n",
        "      num_return_sequences=1,\n",
        "      eos_token_id=128258,\n",
        "      # end_token_id=128009\n",
        "  )\n",
        "\n",
        "# generated_ids = torch.cat([generated_ids, torch.tensor([[128262]]).to(\"cuda\")], dim=1) # EOAI"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Convert output to speech\n",
        "%%capture\n",
        "token_to_find = 128257\n",
        "token_to_remove = 128258\n",
        "\n",
        "# Check if the token exists in the tensor\n",
        "token_indices = (generated_ids == token_to_find).nonzero(as_tuple=True)\n",
        "\n",
        "if len(token_indices[1]) > 0:\n",
        "    last_occurrence_idx = token_indices[1][-1].item()\n",
        "    cropped_tensor = generated_ids[:, last_occurrence_idx+1:]\n",
        "else:\n",
        "    cropped_tensor = generated_ids\n",
        "\n",
        "mask = cropped_tensor != token_to_remove\n",
        "processed_rows = []\n",
        "for row in cropped_tensor:\n",
        "    # Apply the mask to each row\n",
        "    masked_row = row[row != token_to_remove]\n",
        "    processed_rows.append(masked_row)\n",
        "\n",
        "code_lists = []\n",
        "for row in processed_rows:\n",
        "    # row is a 1D tensor with its own length\n",
        "    row_length = row.size(0)\n",
        "    new_length = (row_length // 7) * 7  # largest multiple of 7 that fits in this row\n",
        "    trimmed_row = row[:new_length]\n",
        "    trimmed_row = [t - 128266 for t in trimmed_row]\n",
        "    code_lists.append(trimmed_row)\n",
        "\n",
        "def redistribute_codes(code_list):\n",
        "  layer_1 = []\n",
        "  layer_2 = []\n",
        "  layer_3 = []\n",
        "  for i in range((len(code_list)+1)//7):\n",
        "    layer_1.append(code_list[7*i])\n",
        "    layer_2.append(code_list[7*i+1]-4096)\n",
        "    layer_3.append(code_list[7*i+2]-(2*4096))\n",
        "    layer_3.append(code_list[7*i+3]-(3*4096))\n",
        "    layer_2.append(code_list[7*i+4]-(4*4096))\n",
        "    layer_3.append(code_list[7*i+5]-(5*4096))\n",
        "    layer_3.append(code_list[7*i+6]-(6*4096))\n",
        "  codes = [torch.tensor(layer_1).unsqueeze(0),\n",
        "         torch.tensor(layer_2).unsqueeze(0),\n",
        "         torch.tensor(layer_3).unsqueeze(0)]\n",
        "  audio_hat = snac_model.decode(codes)\n",
        "  return audio_hat\n",
        "\n",
        "my_samples = []\n",
        "for code_list in code_lists:\n",
        "  samples = redistribute_codes(code_list)\n",
        "  my_samples.append(samples)"
      ],
      "metadata": {
        "id": "lV49oiPFpbXL",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Display Speech\n",
        "from IPython.display import Audio, display\n",
        "for samples in my_samples:\n",
        "  display(Audio(samples.detach().squeeze().to(\"cpu\").numpy(), rate=24000))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "JuwkHqU4piMJ",
        "outputId": "c27f4f08-f57e-44be-9b21-3d381b3dd452",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'my_samples' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-22d6a3267f3a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0msamples\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmy_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAudio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m24000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'my_samples' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uQoWOaFC1EDi"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}